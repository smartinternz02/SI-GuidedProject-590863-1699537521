{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**"
      ],
      "metadata": {
        "id": "kdPEFxAcfhp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import load_model\n"
      ],
      "metadata": {
        "id": "WazWPgNEEYHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Creation**"
      ],
      "metadata": {
        "id": "Aoh5wH4hfWjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training & Testing Paths\n",
        "\n",
        "trainPath = '/content/dataset/train'\n",
        "testPath = '/content/dataset/test'\n"
      ],
      "metadata": {
        "id": "yCpVEu0eEgfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Configure VGG16 Model\n",
        "\n",
        "\n",
        "imageSize = (224, 224)  # Define image size\n",
        "vgg = VGG16(input_shape=imageSize + (3,), weights='imagenet', include_top=False)\n",
        "\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(vgg.output)\n",
        "prediction = Dense(8, activation='softmax')(x)  # 8 classes for tea leaf diseases\n",
        "\n",
        "model = Model(inputs=vgg.input, outputs=prediction)\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "hY-_yhgAH-w6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa025c69-2000-40cc-cf70-667ed0e88039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageDataGenerator Configuration for Data Augmentation:\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    trainPath,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    testPath,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "id": "Vxqkyw6rIGJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0faa93-1a5c-4eab-d4b7-02cdd2309040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 706 images belonging to 8 classes.\n",
            "Found 179 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Training**"
      ],
      "metadata": {
        "id": "gxfWZdWLfbud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "\n",
        "r = model.fit(\n",
        "    training_set,\n",
        "    validation_data=test_set,\n",
        "    epochs=30\n",
        ")\n"
      ],
      "metadata": {
        "id": "tK9rWE32IPQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bdd76d-8ed9-4eb4-f37e-d0caf1756217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "23/23 [==============================] - 549s 24s/step - loss: 2.1092 - accuracy: 0.3258 - val_loss: 1.4178 - val_accuracy: 0.4972\n",
            "Epoch 2/30\n",
            "23/23 [==============================] - 43s 2s/step - loss: 1.1890 - accuracy: 0.5907 - val_loss: 1.0249 - val_accuracy: 0.6145\n",
            "Epoch 3/30\n",
            "23/23 [==============================] - 43s 2s/step - loss: 0.7187 - accuracy: 0.7436 - val_loss: 0.9063 - val_accuracy: 0.6704\n",
            "Epoch 4/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.5722 - accuracy: 0.7932 - val_loss: 0.9021 - val_accuracy: 0.6704\n",
            "Epoch 5/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.6234 - accuracy: 0.7960 - val_loss: 0.9940 - val_accuracy: 0.6369\n",
            "Epoch 6/30\n",
            "23/23 [==============================] - 42s 2s/step - loss: 0.4563 - accuracy: 0.8399 - val_loss: 0.7574 - val_accuracy: 0.6983\n",
            "Epoch 7/30\n",
            "23/23 [==============================] - 43s 2s/step - loss: 0.3418 - accuracy: 0.8796 - val_loss: 0.6756 - val_accuracy: 0.7430\n",
            "Epoch 8/30\n",
            "23/23 [==============================] - 42s 2s/step - loss: 0.3102 - accuracy: 0.9093 - val_loss: 0.8102 - val_accuracy: 0.6927\n",
            "Epoch 9/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.3453 - accuracy: 0.8754 - val_loss: 0.8511 - val_accuracy: 0.7039\n",
            "Epoch 10/30\n",
            "23/23 [==============================] - 42s 2s/step - loss: 0.2587 - accuracy: 0.9235 - val_loss: 0.8129 - val_accuracy: 0.7486\n",
            "Epoch 11/30\n",
            "23/23 [==============================] - 40s 2s/step - loss: 0.2543 - accuracy: 0.9263 - val_loss: 0.6217 - val_accuracy: 0.7709\n",
            "Epoch 12/30\n",
            "23/23 [==============================] - 43s 2s/step - loss: 0.2035 - accuracy: 0.9533 - val_loss: 0.6152 - val_accuracy: 0.7877\n",
            "Epoch 13/30\n",
            "23/23 [==============================] - 46s 2s/step - loss: 0.1850 - accuracy: 0.9504 - val_loss: 0.6710 - val_accuracy: 0.7207\n",
            "Epoch 14/30\n",
            "23/23 [==============================] - 40s 2s/step - loss: 0.2003 - accuracy: 0.9419 - val_loss: 0.6332 - val_accuracy: 0.7430\n",
            "Epoch 15/30\n",
            "23/23 [==============================] - 43s 2s/step - loss: 0.1457 - accuracy: 0.9688 - val_loss: 0.5759 - val_accuracy: 0.7709\n",
            "Epoch 16/30\n",
            "23/23 [==============================] - 43s 2s/step - loss: 0.1181 - accuracy: 0.9844 - val_loss: 0.5626 - val_accuracy: 0.7765\n",
            "Epoch 17/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.1125 - accuracy: 0.9844 - val_loss: 0.5693 - val_accuracy: 0.7542\n",
            "Epoch 18/30\n",
            "23/23 [==============================] - 40s 2s/step - loss: 0.1013 - accuracy: 0.9844 - val_loss: 0.5735 - val_accuracy: 0.7821\n",
            "Epoch 19/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.0920 - accuracy: 0.9830 - val_loss: 0.5342 - val_accuracy: 0.8212\n",
            "Epoch 20/30\n",
            "23/23 [==============================] - 42s 2s/step - loss: 0.0924 - accuracy: 0.9887 - val_loss: 0.5630 - val_accuracy: 0.7877\n",
            "Epoch 21/30\n",
            "23/23 [==============================] - 40s 2s/step - loss: 0.0840 - accuracy: 0.9901 - val_loss: 0.6590 - val_accuracy: 0.7709\n",
            "Epoch 22/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.1034 - accuracy: 0.9830 - val_loss: 0.5937 - val_accuracy: 0.7709\n",
            "Epoch 23/30\n",
            "23/23 [==============================] - 42s 2s/step - loss: 0.0789 - accuracy: 0.9943 - val_loss: 0.5580 - val_accuracy: 0.8045\n",
            "Epoch 24/30\n",
            "23/23 [==============================] - 42s 2s/step - loss: 0.0777 - accuracy: 0.9873 - val_loss: 0.6254 - val_accuracy: 0.7709\n",
            "Epoch 25/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.0766 - accuracy: 0.9915 - val_loss: 0.6385 - val_accuracy: 0.7207\n",
            "Epoch 26/30\n",
            "23/23 [==============================] - 40s 2s/step - loss: 0.0608 - accuracy: 0.9958 - val_loss: 0.5898 - val_accuracy: 0.7877\n",
            "Epoch 27/30\n",
            "23/23 [==============================] - 41s 2s/step - loss: 0.0595 - accuracy: 0.9929 - val_loss: 0.6158 - val_accuracy: 0.7654\n",
            "Epoch 28/30\n",
            "23/23 [==============================] - 47s 2s/step - loss: 0.0729 - accuracy: 0.9929 - val_loss: 0.6359 - val_accuracy: 0.7933\n",
            "Epoch 29/30\n",
            "23/23 [==============================] - 40s 2s/step - loss: 0.0823 - accuracy: 0.9788 - val_loss: 0.6609 - val_accuracy: 0.7598\n",
            "Epoch 30/30\n",
            "23/23 [==============================] - 43s 2s/step - loss: 0.0660 - accuracy: 0.9929 - val_loss: 0.5963 - val_accuracy: 0.7654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "\n",
        "model.save(\"vgg_16_tea_leaf_disease.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpbTXBMPeOC-",
        "outputId": "5d0d3744-57b6-4fd3-cd19-cbbaade3ac34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing The Model**"
      ],
      "metadata": {
        "id": "UV4Vrni3YfvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and preparing test image\n",
        "# Chosen leaf disease image is Antracnose\n",
        "\n",
        "img_path = \"/content/drive/MyDrive/Sem 5 Externship/project/tea_sickness_dataset/test/Anthracnose/IMG_20220503_145703.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "x = np.expand_dims(x, axis=0)\n",
        "img_data = preprocess_input(x)\n"
      ],
      "metadata": {
        "id": "xEbPyTU4Ylg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = load_model('/content/drive/MyDrive/Sem 5 Externship/project/vgg_16_tea_leaf_disease.h5')\n",
        "\n",
        "# Make a prediction on the image\n",
        "output = np.argmax(model.predict(img_data), axis=1)\n",
        "\n",
        "# Mapping the index to class names\n",
        "index = ['Antracnose', 'algal leaf', 'bird eye spot', 'brown light', 'gray light', 'healthy', 'red leaf spot', 'white spot']\n",
        "result = index[output[0]]\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "43HkFSbdaKVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}